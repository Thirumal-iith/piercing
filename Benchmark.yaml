name: Generate Benchmark Scenario
description: Creates a continual learning benchmark scenario using Avalanche.

inputs:
  - {}

outputs:
  - {name: scenario_path, type: Path}

implementation:
  container:
    image: python:3.10
    command:
    - sh
    - -c
    - |
      pip install avalanche-lib==0.6
      python3 -u -c """
      import argparse, pickle
      from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR10, SplitCIFAR100, SplitTinyImageNet

      benchmark_name = "splitmnist"
      n_experiences = 5
      return_task_id = False

      def get_benchmark(name, n_exp, return_id):
          name = name.lower()
          if name == 'splitmnist':
              return SplitMNIST(n_experiences=n_exp, return_task_id=return_id)
          elif name == 'splitcifar10':
              return SplitCIFAR10(n_experiences=n_exp, return_task_id=return_id)
          elif name == 'splitcifar100':
              return SplitCIFAR100(n_experiences=n_exp, return_task_id=return_id)
          elif name == 'splittinyimagenet':
              return SplitTinyImageNet(n_experiences=n_exp, return_task_id=return_id)
          else:
              raise ValueError(f"Unknown benchmark: {name}")

      parser = argparse.ArgumentParser()
      parser.add_argument('--scenario_path', type=str, required=True)
      args = parser.parse_args()

      scenario = get_benchmark(benchmark_name, n_experiences, return_task_id)
      with open("scenario.pkl", "wb") as f:
          pickle.dump(scenario, f)
      """
    args:
    - --scenario_path
    - {outputPath: scenario_path}